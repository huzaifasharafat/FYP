{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becfc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vname = \"test2.0Ch12 20240311121534.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c73754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!yolo track source=vname save=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0aba1a",
   "metadata": {},
   "source": [
    "track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "# Dictionary to store tracking history with default empty lists\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Load the YOLO model with segmentation capabilities\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(\"Ch12 20240312083049.mp4\")\n",
    "\n",
    "# Retrieve video properties: width, height, and frames per second\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize video writer to save the output video with the specified properties\n",
    "out = cv2.VideoWriter(\"instance-segmentation-object-tracking.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, im0 = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    # Create an annotator object to draw on the frame\n",
    "    annotator = Annotator(im0, line_width=2)\n",
    "\n",
    "    # Perform object tracking on the current frame\n",
    "    #results = model.track(im0, persist=True)\n",
    "    results = model(source=im0, conf=0.4)\n",
    "    \n",
    "\n",
    "    # Check if tracking IDs and masks are present in the results\n",
    "    if results[0].boxes.id is not None and results[0].masks is not None:\n",
    "        # Extract masks and tracking IDs\n",
    "        masks = results[0].masks.xy\n",
    "        track_ids = results[0].boxes.id.int().c().tolist()\n",
    "\n",
    "        # Annotate each mask with its corresponding tracking ID and color\n",
    "        for mask, track_id in zip(masks, track_ids):\n",
    "            annotator.seg_bbox(mask=mask, mask_color=colors(track_id, True), track_label=str(track_id))\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(im0)\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"instance-segmentation-object-tracking\", im0)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video writer and capture objects, and close all OpenCV windows\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaadf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vname = \"test1.2Ch18 20240311101732.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = YOLO(\"yolov8l.pt\" )\n",
    "\n",
    "#results = model(source=\"test1Ch18 20240311101732.mp4\", show=True ,  save = True)\n",
    "\n",
    "cap = cv2.VideoCapture(vname)\n",
    "frames_tracked = []\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        #cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "        send_video_frame(annotated_frame)\n",
    "        \n",
    "        #frames_tracked.append(annotated_frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        #if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            #break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed47b411",
   "metadata": {},
   "source": [
    "sendin data to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b646dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncap.release()\\ncv2.destroyAllWindows()\\nsio.disconnect()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import socketio\n",
    "import cv2\n",
    "import base64\n",
    "import numpy as np\n",
    "\n",
    "# Create a Socket.IO client\n",
    "sio = socketio.Client()\n",
    "\n",
    "@sio.event\n",
    "def connect():\n",
    "    print('Connection established')\n",
    "\n",
    "@sio.event\n",
    "def disconnect():\n",
    "    print('Disconnected from server')\n",
    "\n",
    "@sio.event\n",
    "def video_frame(data):\n",
    "    print('Received video frame')\n",
    "    try:\n",
    "        # Debugging: Print the size of the data received\n",
    "        print(f'Size of received data: {len(data)}')\n",
    "        \n",
    "        # Deserialize the received frame\n",
    "        np_data = np.frombuffer(base64.b64decode(data), np.uint8)\n",
    "        \n",
    "        # Debugging: Check the shape of np_data before decoding\n",
    "        print(f'Shape of np_data: {np_data.shape}')\n",
    "        \n",
    "        frame = cv2.imdecode(np_data, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if frame is None:\n",
    "            print('Error: Frame is None after imdecode')\n",
    "        else:\n",
    "            cv2.imshow('Received Video Frame', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            sio.disconnect()\n",
    "    except Exception as e:\n",
    "        print(f'Error in video_frame handler: {e}')\n",
    "\n",
    "def send_video_frame(frame):\n",
    "    try:\n",
    "        # Serialize the frame to send it over the socket\n",
    "        _, buffer = cv2.imencode('.jpg', frame)\n",
    "        \n",
    "        # Debugging: Print the size of the buffer\n",
    "        print(f'Size of buffer: {buffer.size}')\n",
    "        \n",
    "        frame_data = base64.b64encode(buffer).decode('utf-8')\n",
    "        sio.emit('video_frame', frame_data)\n",
    "    except Exception as e:\n",
    "        print(f'Error in send_video_frame: {e}')\n",
    "\n",
    "# Connect to the server\n",
    "sio.connect('http://localhost:3000')\n",
    "\n",
    "# Capture video from webcam and send frames to the server\n",
    "'''\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    send_video_frame(frame)\n",
    "'''\n",
    "'''\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "sio.disconnect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a233675",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = Image.fromarray(cv2.cvtColor(frames_tracked[0], cv2.COLOR_BGR2RGB)).size\n",
    "fourcc = cv2.VideoWriter_fourcc(*'FMP4')    \n",
    "video_tracked = cv2.VideoWriter('YOLO2tracked'+vname, fourcc, 25.0, dim)\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(frame)\n",
    "video_tracked.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GuardianWatchj",
   "language": "python",
   "name": "guardianwatchj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
